{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Information Retrieval System\n",
    "Mencakup:\n",
    "- Text Preprocessing dan Tokenisasi (15%)\n",
    "- Representasi Dokumen (Bag of Words) (15%)\n",
    "- Implementasi Indexing dengan Whoosh (25%)\n",
    "- Pencarian dan Ranking menggunakan Cosine Similarity (25%)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict, Counter\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import scoring\n",
    "\n",
    "# Import Sastrawi untuk stemming Bahasa Indonesia\n",
    "try:\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    SASTRAWI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SASTRAWI_AVAILABLE = False\n",
    "    print(\"Warning: Sastrawi not installed. Install with: pip install Sastrawi\")\n",
    "    print(\"Using simple stemming as fallback.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Text Preprocessing dan Tokenisasi (15%)\"\"\"\n",
    "    \n",
    "    def __init__(self, use_sastrawi=True):\n",
    "        # Daftar stopwords bahasa Indonesia dan Inggris\n",
    "        self.stopwords = set([\n",
    "            'dan', 'di', 'ke', 'dari', 'yang', 'untuk', 'pada', 'dengan', 'adalah',\n",
    "            'ini', 'itu', 'atau', 'se', 'akan', 'telah', 'ada', 'sebagai',\n",
    "            'a', 'an', 'the', 'is', 'are', 'was', 'were', 'and', 'or', 'but', \n",
    "            'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'\n",
    "        ])\n",
    "        \n",
    "        # Inisialisasi Sastrawi Stemmer\n",
    "        self.use_sastrawi = use_sastrawi and SASTRAWI_AVAILABLE\n",
    "        if self.use_sastrawi:\n",
    "            factory = StemmerFactory()\n",
    "            self.sastrawi_stemmer = factory.create_stemmer()\n",
    "            print(\"✓ Sastrawi Stemmer initialized\\n\")\n",
    "        else:\n",
    "            self.sastrawi_stemmer = None\n",
    "            print(\"✗ Using simple stemming (Sastrawi not available)\\n\")\n",
    "    \n",
    "    def case_folding(self, text):\n",
    "        \"\"\"Mengubah semua huruf menjadi lowercase\"\"\"\n",
    "        return text.lower()\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"Menghapus tanda baca\"\"\"\n",
    "        return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Memecah teks menjadi token (kata-kata)\"\"\"\n",
    "        tokens = text.split()\n",
    "        return [token for token in tokens if token]\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Menghapus stopwords\"\"\"\n",
    "        return [token for token in tokens if token not in self.stopwords]\n",
    "    \n",
    "    def stemming_simple(self, token):\n",
    "        \"\"\"Stemming sederhana (suffix removal) - fallback\"\"\"\n",
    "        # Stemming untuk bahasa Inggris\n",
    "        if token.endswith('ing'):\n",
    "            return token[:-3]\n",
    "        elif token.endswith('ed'):\n",
    "            return token[:-2]\n",
    "        elif token.endswith('s') and len(token) > 3:\n",
    "            return token[:-1]\n",
    "        \n",
    "        # Stemming untuk bahasa Indonesia\n",
    "        elif token.endswith('kan'):\n",
    "            return token[:-3]\n",
    "        elif token.endswith('an'):\n",
    "            return token[:-2]\n",
    "        elif token.endswith('i') and len(token) > 3:\n",
    "            return token[:-1]\n",
    "        \n",
    "        return token\n",
    "    \n",
    "    def stemming(self, tokens):\n",
    "        \"\"\"Stemming menggunakan Sastrawi (untuk Bahasa Indonesia)\"\"\"\n",
    "        if self.use_sastrawi:\n",
    "            # Sastrawi dapat memproses list atau string\n",
    "            stemmed_tokens = [self.sastrawi_stemmer.stem(token) for token in tokens]\n",
    "            return stemmed_tokens\n",
    "        else:\n",
    "            # Fallback ke simple stemming\n",
    "            return [self.stemming_simple(token) for token in tokens]\n",
    "    \n",
    "    def preprocess(self, text, verbose=False):\n",
    "        \"\"\"Pipeline preprocessing lengkap\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Original Text: {text}\")\n",
    "        \n",
    "        # 1. Case folding\n",
    "        text = self.case_folding(text)\n",
    "        if verbose:\n",
    "            print(f\"Case Folding: {text}\")\n",
    "        \n",
    "        # 2. Remove punctuation\n",
    "        text = self.remove_punctuation(text)\n",
    "        if verbose:\n",
    "            print(f\"Remove Punctuation: {text}\")\n",
    "        \n",
    "        # 3. Tokenization\n",
    "        tokens = self.tokenize(text)\n",
    "        if verbose:\n",
    "            print(f\"Tokenization ({len(tokens)} tokens): {tokens}\")\n",
    "        \n",
    "        # 4. Remove stopwords\n",
    "        tokens_before_stopword = tokens.copy()\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        if verbose:\n",
    "            removed = set(tokens_before_stopword) - set(tokens)\n",
    "            print(f\"Remove Stopwords ({len(tokens)} remaining): {tokens}\")\n",
    "            if removed:\n",
    "                print(f\"  Removed: {removed}\")\n",
    "        \n",
    "        # 5. Stemming dengan Sastrawi\n",
    "        tokens_before_stem = tokens.copy()\n",
    "        tokens = self.stemming(tokens)\n",
    "        if verbose:\n",
    "            print(f\"Stemming ({len(tokens)} tokens): {tokens}\")\n",
    "            # Tampilkan perubahan stemming\n",
    "            changes = [(before, after) for before, after in zip(tokens_before_stem, tokens) if before != after]\n",
    "            if changes:\n",
    "                print(f\"  Stemming changes:\")\n",
    "                for before, after in changes:\n",
    "                    print(f\"    {before} → {after}\")\n",
    "        \n",
    "        return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BagOfWords:\n",
    "    \"\"\"Representasi Dokumen (Bag of Words) (15%)\"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.vocabulary = set()\n",
    "        self.doc_vectors = []\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Membuat vocabulary dari dokumen\"\"\"\n",
    "        for doc in documents:\n",
    "            tokens = self.preprocessor.preprocess(doc)\n",
    "            self.vocabulary.update(tokens)\n",
    "        \n",
    "        self.vocabulary = sorted(list(self.vocabulary))\n",
    "        print(f\"\\nVocabulary ({len(self.vocabulary)} terms): {self.vocabulary}\\n\")\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Mengubah dokumen menjadi vector BoW\"\"\"\n",
    "        self.doc_vectors = []\n",
    "        \n",
    "        for idx, doc in enumerate(documents):\n",
    "            tokens = self.preprocessor.preprocess(doc, verbose=False)\n",
    "            token_counts = Counter(tokens)\n",
    "            \n",
    "            # Membuat vector berdasarkan vocabulary\n",
    "            vector = [token_counts.get(term, 0) for term in self.vocabulary]\n",
    "            self.doc_vectors.append(vector)\n",
    "            \n",
    "            print(f\"Document {idx + 1} Vector:\")\n",
    "            print(f\"  Text: {doc}\")\n",
    "            print(f\"  Tokens: {tokens}\")\n",
    "            print(f\"  Vector: {vector}\")\n",
    "            print()\n",
    "        \n",
    "        return self.doc_vectors\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"Fit dan transform sekaligus\"\"\"\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)\n",
    "    \n",
    "    def get_term_frequency(self, doc_idx, term):\n",
    "        \"\"\"Mendapatkan frekuensi term dalam dokumen\"\"\"\n",
    "        if term in self.vocabulary:\n",
    "            term_idx = self.vocabulary.index(term)\n",
    "            return self.doc_vectors[doc_idx][term_idx]\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhooshIndexer:\n",
    "    \"\"\"Implementasi Indexing dengan Whoosh (25%)\"\"\"\n",
    "    \n",
    "    def __init__(self, index_dir=\"indexdir\"):\n",
    "        self.index_dir = index_dir\n",
    "        self.schema = Schema(\n",
    "            doc_id=ID(stored=True),\n",
    "            content=TEXT(stored=True)\n",
    "        )\n",
    "        self.ix = None\n",
    "    \n",
    "    def create_index(self):\n",
    "        \"\"\"Membuat index baru\"\"\"\n",
    "        # Hapus index lama jika ada\n",
    "        if os.path.exists(self.index_dir):\n",
    "            shutil.rmtree(self.index_dir)\n",
    "        \n",
    "        os.mkdir(self.index_dir)\n",
    "        self.ix = create_in(self.index_dir, self.schema)\n",
    "        print(f\"Index created in '{self.index_dir}'\")\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"Menambahkan dokumen ke index\"\"\"\n",
    "        writer = self.ix.writer()\n",
    "        \n",
    "        for idx, doc in enumerate(documents):\n",
    "            writer.add_document(\n",
    "                doc_id=str(idx + 1),\n",
    "                content=doc\n",
    "            )\n",
    "            print(f\"Added Document {idx + 1}: {doc[:50]}...\")\n",
    "        \n",
    "        writer.commit()\n",
    "        print(f\"\\n{len(documents)} documents indexed successfully!\\n\")\n",
    "    \n",
    "    def search_whoosh(self, query_text, limit=10):\n",
    "        \"\"\"Mencari menggunakan Whoosh\"\"\"\n",
    "        if not self.ix:\n",
    "            self.ix = open_dir(self.index_dir)\n",
    "        \n",
    "        with self.ix.searcher(weighting=scoring.BM25F()) as searcher:\n",
    "            query = QueryParser(\"content\", self.ix.schema).parse(query_text)\n",
    "            results = searcher.search(query, limit=limit)\n",
    "            \n",
    "            print(f\"Whoosh Search Results for: '{query_text}'\")\n",
    "            print(f\"Found {len(results)} results\\n\")\n",
    "            \n",
    "            search_results = []\n",
    "            for hit in results:\n",
    "                search_results.append({\n",
    "                    'doc_id': hit['doc_id'],\n",
    "                    'content': hit['content'],\n",
    "                    'score': hit.score\n",
    "                })\n",
    "                print(f\"Doc ID: {hit['doc_id']}\")\n",
    "                print(f\"Score: {hit.score:.4f}\")\n",
    "                print(f\"Content: {hit['content'][:100]}...\")\n",
    "                print()\n",
    "            \n",
    "            return search_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CosineSimilarityRanker:\n",
    "    \"\"\"Pencarian dan Ranking menggunakan Cosine Similarity (25%)\"\"\"\n",
    "    \n",
    "    def __init__(self, bow_model):\n",
    "        self.bow_model = bow_model\n",
    "    \n",
    "    def calculate_cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"Menghitung cosine similarity antara dua vector\"\"\"\n",
    "        # Dot product\n",
    "        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "        \n",
    "        # Magnitude\n",
    "        magnitude1 = math.sqrt(sum(a * a for a in vec1))\n",
    "        magnitude2 = math.sqrt(sum(b * b for b in vec2))\n",
    "        \n",
    "        # Cosine similarity\n",
    "        if magnitude1 == 0 or magnitude2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    def search(self, query, documents):\n",
    "        \"\"\"Mencari dokumen relevan menggunakan cosine similarity\"\"\"\n",
    "        # Preprocess query\n",
    "        query_tokens = self.bow_model.preprocessor.preprocess(query, verbose=False)\n",
    "        query_counts = Counter(query_tokens)\n",
    "        \n",
    "        # Buat query vector\n",
    "        query_vector = [query_counts.get(term, 0) for term in self.bow_model.vocabulary]\n",
    "        \n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"Query Tokens: {query_tokens}\")\n",
    "        print(f\"Query Vector: {query_vector}\\n\")\n",
    "        \n",
    "        # Hitung similarity untuk setiap dokumen\n",
    "        results = []\n",
    "        for idx, doc_vector in enumerate(self.bow_model.doc_vectors):\n",
    "            similarity = self.calculate_cosine_similarity(query_vector, doc_vector)\n",
    "            results.append({\n",
    "                'doc_id': idx + 1,\n",
    "                'content': documents[idx],\n",
    "                'similarity': similarity,\n",
    "                'score_percent': similarity * 100\n",
    "            })\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        \n",
    "        # Filter hasil dengan similarity > 0\n",
    "        results = [r for r in results if r['similarity'] > 0]\n",
    "        \n",
    "        print(\"Cosine Similarity Ranking:\")\n",
    "        print(\"-\" * 80)\n",
    "        for rank, result in enumerate(results, 1):\n",
    "            print(f\"Rank {rank}:\")\n",
    "            print(f\"  Doc ID: {result['doc_id']}\")\n",
    "            print(f\"  Similarity: {result['similarity']:.4f} ({result['score_percent']:.2f}%)\")\n",
    "            print(f\"  Content: {result['content'][:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INFORMATION RETRIEVAL SYSTEM WITH SASTRAWI STEMMER\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "1. TEXT PREPROCESSING DAN TOKENISASI (15%)\n",
      "================================================================================\n",
      "✓ Sastrawi Stemmer initialized\n",
      "\n",
      "\n",
      "Sample Text: Pemrosesan bahasa alami adalah cabang dari kecerdasan buatan\n",
      "\n",
      "Original Text: Pemrosesan bahasa alami adalah cabang dari kecerdasan buatan\n",
      "Case Folding: pemrosesan bahasa alami adalah cabang dari kecerdasan buatan\n",
      "Remove Punctuation: pemrosesan bahasa alami adalah cabang dari kecerdasan buatan\n",
      "Tokenization (8 tokens): ['pemrosesan', 'bahasa', 'alami', 'adalah', 'cabang', 'dari', 'kecerdasan', 'buatan']\n",
      "Remove Stopwords (6 remaining): ['pemrosesan', 'bahasa', 'alami', 'cabang', 'kecerdasan', 'buatan']\n",
      "  Removed: {'adalah', 'dari'}\n",
      "Stemming (6 tokens): ['pemrosesan', 'bahasa', 'alami', 'cabang', 'cerdas', 'buat']\n",
      "  Stemming changes:\n",
      "    kecerdasan → cerdas\n",
      "    buatan → buat\n",
      "\n",
      "================================================================================\n",
      "2. REPRESENTASI DOKUMEN - BAG OF WORDS (15%)\n",
      "================================================================================\n",
      "\n",
      "Vocabulary (32 terms): ['ajar', 'alami', 'algoritma', 'analis', 'bahasa', 'bantu', 'besar', 'buat', 'cabang', 'cerdas', 'dalam', 'dapat', 'data', 'dataset', 'dokumen', 'guna', 'ilmu', 'informasi', 'jumlah', 'kembali', 'latih', 'mesin', 'model', 'pemrograman', 'pemrosesan', 'perlu', 'populer', 'proses', 'python', 'relevan', 'sistem', 'temu']\n",
      "\n",
      "Document 1 Vector:\n",
      "  Text: Pemrosesan bahasa alami adalah cabang dari kecerdasan buatan\n",
      "  Tokens: ['pemrosesan', 'bahasa', 'alami', 'cabang', 'cerdas', 'buat']\n",
      "  Vector: [0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Document 2 Vector:\n",
      "  Text: Algoritma pembelajaran mesin dapat memproses dan menganalisis dataset besar\n",
      "  Tokens: ['algoritma', 'ajar', 'mesin', 'dapat', 'proses', 'analis', 'dataset', 'besar']\n",
      "  Vector: [1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "\n",
      "Document 3 Vector:\n",
      "  Text: Sistem temu kembali informasi membantu pengguna menemukan dokumen yang relevan\n",
      "  Tokens: ['sistem', 'temu', 'kembali', 'informasi', 'bantu', 'guna', 'temu', 'dokumen', 'relevan']\n",
      "  Vector: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2]\n",
      "\n",
      "Document 4 Vector:\n",
      "  Text: Python adalah bahasa pemrograman populer untuk ilmu data\n",
      "  Tokens: ['python', 'bahasa', 'pemrograman', 'populer', 'ilmu', 'data']\n",
      "  Vector: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "\n",
      "Document 5 Vector:\n",
      "  Text: Model pembelajaran mendalam memerlukan data pelatihan dalam jumlah besar\n",
      "  Tokens: ['model', 'ajar', 'dalam', 'perlu', 'data', 'latih', 'dalam', 'jumlah', 'besar']\n",
      "  Vector: [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "================================================================================\n",
      "3. IMPLEMENTASI INDEXING DENGAN WHOOSH (25%)\n",
      "================================================================================\n",
      "\n",
      "Index created in 'indexdir'\n",
      "Added Document 1: Pemrosesan bahasa alami adalah cabang dari kecerda...\n",
      "Added Document 2: Algoritma pembelajaran mesin dapat memproses dan m...\n",
      "Added Document 3: Sistem temu kembali informasi membantu pengguna me...\n",
      "Added Document 4: Python adalah bahasa pemrograman populer untuk ilm...\n",
      "Added Document 5: Model pembelajaran mendalam memerlukan data pelati...\n",
      "\n",
      "5 documents indexed successfully!\n",
      "\n",
      "Whoosh Search Results for: 'pembelajaran mesin'\n",
      "Found 1 results\n",
      "\n",
      "Doc ID: 2\n",
      "Score: 3.3955\n",
      "Content: Algoritma pembelajaran mesin dapat memproses dan menganalisis dataset besar...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "4. PENCARIAN DAN RANKING - COSINE SIMILARITY (25%)\n",
      "================================================================================\n",
      "\n",
      "Query: 'pembelajaran mesin data'\n",
      "Query Tokens: ['ajar', 'mesin', 'data']\n",
      "Query Vector: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Cosine Similarity Ranking:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 1:\n",
      "  Doc ID: 2\n",
      "  Similarity: 0.4082 (40.82%)\n",
      "  Content: Algoritma pembelajaran mesin dapat memproses dan menganalisis dataset besar...\n",
      "\n",
      "Rank 2:\n",
      "  Doc ID: 5\n",
      "  Similarity: 0.3482 (34.82%)\n",
      "  Content: Model pembelajaran mendalam memerlukan data pelatihan dalam jumlah besar...\n",
      "\n",
      "Rank 3:\n",
      "  Doc ID: 4\n",
      "  Similarity: 0.2357 (23.57%)\n",
      "  Content: Python adalah bahasa pemrograman populer untuk ilmu data...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DEMO PENCARIAN TAMBAHAN\n",
      "================================================================================\n",
      "\n",
      "--- Query: 'sistem informasi' ---\n",
      "\n",
      "Query: 'sistem informasi'\n",
      "Query Tokens: ['sistem', 'informasi']\n",
      "Query Vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "Cosine Similarity Ranking:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 1:\n",
      "  Doc ID: 3\n",
      "  Similarity: 0.4264 (42.64%)\n",
      "  Content: Sistem temu kembali informasi membantu pengguna menemukan dokumen yang relevan...\n",
      "\n",
      "\n",
      "--- Query: 'pemrograman python' ---\n",
      "\n",
      "Query: 'pemrograman python'\n",
      "Query Tokens: ['pemrograman', 'python']\n",
      "Query Vector: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "\n",
      "Cosine Similarity Ranking:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 1:\n",
      "  Doc ID: 4\n",
      "  Similarity: 0.5774 (57.74%)\n",
      "  Content: Python adalah bahasa pemrograman populer untuk ilmu data...\n",
      "\n",
      "\n",
      "--- Query: 'kecerdasan buatan' ---\n",
      "\n",
      "Query: 'kecerdasan buatan'\n",
      "Query Tokens: ['cerdas', 'buat']\n",
      "Query Vector: [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Cosine Similarity Ranking:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 1:\n",
      "  Doc ID: 1\n",
      "  Similarity: 0.5774 (57.74%)\n",
      "  Content: Pemrosesan bahasa alami adalah cabang dari kecerdasan buatan...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PERBANDINGAN METODE & KEUNGGULAN SASTRAWI\n",
      "================================================================================\n",
      "\n",
      "Whoosh (BM25F) vs Cosine Similarity:\n",
      "- Whoosh: Menggunakan algoritma BM25F yang mempertimbangkan TF-IDF\n",
      "- Cosine Similarity: Mengukur kesamaan sudut antar vector dokumen\n",
      "\n",
      "Keunggulan Sastrawi Stemmer:\n",
      "- ✓ Akurasi tinggi untuk Bahasa Indonesia\n",
      "- ✓ Menangani imbuhan kompleks (me-, ber-, pe-, ter-, dll)\n",
      "- ✓ Kamus kata dasar yang lengkap\n",
      "- ✓ Algoritma Nazief & Adriani yang terbukti efektif\n",
      "\n",
      "Contoh stemming dengan Sastrawi:\n",
      "Kata Asli            → Kata Dasar          \n",
      "------------------------------------------\n",
      "memproses            → proses              \n",
      "pembelajaran         → ajar                \n",
      "menganalisis         → analis              \n",
      "menemukan            → temu                \n",
      "memerlukan           → perlu               \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Demo lengkap sistem Information Retrieval\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"INFORMATION RETRIEVAL SYSTEM WITH SASTRAWI STEMMER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Sample documents (campuran Bahasa Indonesia dan Inggris)\n",
    "    documents = [\n",
    "        \"Pemrosesan bahasa alami adalah cabang dari kecerdasan buatan\",\n",
    "        \"Algoritma pembelajaran mesin dapat memproses dan menganalisis dataset besar\",\n",
    "        \"Sistem temu kembali informasi membantu pengguna menemukan dokumen yang relevan\",\n",
    "        \"Python adalah bahasa pemrograman populer untuk ilmu data\",\n",
    "        \"Model pembelajaran mendalam memerlukan data pelatihan dalam jumlah besar\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"1. TEXT PREPROCESSING DAN TOKENISASI (15%)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    preprocessor = TextPreprocessor(use_sastrawi=True)\n",
    "    sample_text = documents[0]\n",
    "    print(f\"\\nSample Text: {sample_text}\\n\")\n",
    "    tokens = preprocessor.preprocess(sample_text, verbose=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"2. REPRESENTASI DOKUMEN - BAG OF WORDS (15%)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    bow = BagOfWords(preprocessor)\n",
    "    doc_vectors = bow.fit_transform(documents)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"3. IMPLEMENTASI INDEXING DENGAN WHOOSH (25%)\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    indexer = WhooshIndexer()\n",
    "    indexer.create_index()\n",
    "    indexer.add_documents(documents)\n",
    "    \n",
    "    # Search dengan Whoosh\n",
    "    whoosh_query = \"pembelajaran mesin\"\n",
    "    indexer.search_whoosh(whoosh_query)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"4. PENCARIAN DAN RANKING - COSINE SIMILARITY (25%)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ranker = CosineSimilarityRanker(bow)\n",
    "    search_query = \"pembelajaran mesin data\"\n",
    "    results = ranker.search(search_query, documents)\n",
    "    \n",
    "    # Demo tambahan dengan query berbeda\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DEMO PENCARIAN TAMBAHAN\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    queries = [\n",
    "        \"sistem informasi\",\n",
    "        \"pemrograman python\",\n",
    "        \"kecerdasan buatan\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\n--- Query: '{query}' ---\")\n",
    "        results = ranker.search(query, documents)\n",
    "        if not results:\n",
    "            print(\"No relevant documents found.\")\n",
    "\n",
    "\n",
    "    print(\"=== INFORMATION RETRIEVAL SYSTEM ===\")\n",
    "    print(\"[1] load & Index Dataset\")\n",
    "    print(\"[2] Search Query\")\n",
    "    print(\"[3] Exit\")\n",
    "\n",
    "    print(\"PERBANDINGAN METODE & KEUNGGULAN SASTRAWI\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nWhoosh (BM25F) vs Cosine Similarity:\")\n",
    "    print(\"- Whoosh: Menggunakan algoritma BM25F yang mempertimbangkan TF-IDF\")\n",
    "    print(\"- Cosine Similarity: Mengukur kesamaan sudut antar vector dokumen\")\n",
    "    print(\"\\nKeunggulan Sastrawi Stemmer:\")\n",
    "    print(\"- ✓ Akurasi tinggi untuk Bahasa Indonesia\")\n",
    "    print(\"- ✓ Menangani imbuhan kompleks (me-, ber-, pe-, ter-, dll)\")\n",
    "    print(\"- ✓ Kamus kata dasar yang lengkap\")\n",
    "    print(\"- ✓ Algoritma Nazief & Adriani yang terbukti efektif\")\n",
    "    print(\"\\nContoh stemming dengan Sastrawi:\")\n",
    "    test_words = [\"memproses\", \"pembelajaran\", \"menganalisis\", \"menemukan\", \"memerlukan\"]\n",
    "    print(f\"{'Kata Asli':<20} → {'Kata Dasar':<20}\")\n",
    "    print(\"-\" * 42)\n",
    "    for word in test_words:\n",
    "        stemmed = preprocessor.sastrawi_stemmer.stem(word) if preprocessor.use_sastrawi else word\n",
    "        print(f\"{word:<20} → {stemmed:<20}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
